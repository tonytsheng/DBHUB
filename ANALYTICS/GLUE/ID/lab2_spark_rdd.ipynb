{
	"cells": [
		{
			"cell_type": "markdown",
			"metadata": {
				"deletable": false,
				"editable": false,
				"trusted": true
			},
			"source": [
				"\n",
				"# Glue Studio Notebook\n",
				"You are now running a **Glue Studio** notebook; before you can start using your notebook you *must* start an interactive session.\n",
				"\n",
				"## Available Magics\n",
				"|          Magic              |   Type       |                                                                        Description                                                                        |\n",
				"|-----------------------------|--------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
				"| %%configure                 |  Dictionary  |  A json-formatted dictionary consisting of all configuration parameters for a session. Each parameter can be specified here or through individual magics. |\n",
				"| %profile                    |  String      |  Specify a profile in your aws configuration to use as the credentials provider.                                                                          |\n",
				"| %iam_role                   |  String      |  Specify an IAM role to execute your session with.                                                                                                        |\n",
				"| %region                     |  String      |  Specify the AWS region in which to initialize a session                                                                                                  |\n",
				"| %session_id                 |  String      |  Returns the session ID for the running session.                                                                                                          |\n",
				"| %connections                |  List        |  Specify a comma separated list of connections to use in the session.                                                                                     |\n",
				"| %additional_python_modules  |  List        |  Comma separated list of pip packages, s3 paths or private pip arguments.                                                                                 |\n",
				"| %extra_py_files             |  List        |  Comma separated list of additional Python files from S3.                                                                                                 |\n",
				"| %extra_jars                 |  List        |  Comma separated list of additional Jars to include in the cluster.                                                                                       |\n",
				"| %number_of_workers          |  Integer     |  The number of workers of a defined worker_type that are allocated when a job runs. worker_type must be set too.                                          |\n",
				"| %worker_type                |  String      |  Standard, G.1X, *or* G.2X. number_of_workers must be set too. Default is G.1X                                                                            |\n",
				"| %glue_version               |  String      |  The version of Glue to be used by this session. Currently, the only valid options are 2.0 and 3.0 (eg: %glue_version 2.0)                                |\n",
				"| %security_config            |  String      |  Define a security configuration to be used with this session.                                                                                            |\n",
				"| %sql                        |  String      |  Run SQL code. All lines after the initial %%sql magic will be passed as part of the SQL code.                                                            |\n",
				"| %streaming                  |  String      |  Changes the session type to Glue Streaming.                                                                                                              |\n",
				"| %etl                        |  String      |   Changes the session type to Glue ETL.                                                                                                                   |\n",
				"| %status                     |              |  Returns the status of the current Glue session including its duration, configuration and executing user / role.                                          |\n",
				"| %stop_session               |              |  Stops the current session.                                                                                                                               |\n",
				"| %list_sessions              |              |  Lists all currently running sessions by name and ID.                                                                                                     |\n",
				"| %spark_conf                 |  String      |  Specify custom spark configurations for your session. E.g. %spark_conf spark.serializer=org.apache.spark.serializer.KryoSerializer                       |"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"editable": true,
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"import sys\n",
				"from awsglue.transforms import *\n",
				"from awsglue.utils import getResolvedOptions\n",
				"from pyspark.context import SparkContext\n",
				"from awsglue.context import GlueContext\n",
				"from awsglue.job import Job\n",
				"  \n",
				"sc = SparkContext.getOrCreate()\n",
				"glueContext = GlueContext(sc)\n",
				"spark = glueContext.spark_session\n",
				"job = Job(glueContext)\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## What is RDD (Resilient Distributed Dataset)?"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"RDD (Resilient Distributed Dataset) is a fundamental building block of PySpark which is fault-tolerant, immutable distributed collections of objects. \n",
				"Immutable meaning once you create an RDD you cannot change it. Each record in RDD is divided into logical partitions, which can be computed on different nodes of the cluster. "
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"In other words, RDDs are a collection of objects similar to list in Python, with the difference being RDD is computed on several processes scattered across multiple physical servers also called nodes in a cluster while a Python collection lives and process in just one process."
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## PySpark RDD Benefits"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"**In-Memory Processing** - PySpark loads the data from disk and process in memory and keeps the data in memory, this is the main difference between PySpark and Mapreduce (I/O intensive). In between the transformations, we can also cache/persists the RDD in memory to reuse the previous computations."
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"**Immutability** - PySpark RDDs are immutable in nature meaning, once RDDs are created you cannot modify. When we apply transformations on RDD, PySpark creates a new RDD and maintains the RDD Lineage.\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"**Fault Tolerance** - PySpark operates on fault-tolerant data stores on HDFS, S3 e.t.c hence any RDD operation fails, it automatically reloads the data from other partitions. Also, When PySpark applications running on a cluster, PySpark task failures are automatically recovered for a certain number of times (as per the configuration) and finish the application seamlessly."
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"**Lazy Evolution** - PySpark does not evaluate the RDD transformations as they appear/encountered by Driver instead it keeps the all transformations as it encounters(DAG) and evaluates the all transformation when it sees the first RDD action."
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"**Partitioning** - When you create RDD from a data, It by default partitions the elements in a RDD. By default it partitions to the number of cores available."
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Creating RDD"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"RDDs are created primarily in two different ways,\n",
				"\n",
				"* parallelizing an existing collection and\n",
				"* referencing a dataset in an external storage system (HDFS, S3 and many more). "
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"#### Create RDD from parallelize "
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"#Create RDD from parallelize    \n",
				"\n",
				"data = [1,2,3,4,5,6,7,8,9,10,11,12]\n",
				"\n",
				"rdd=spark.sparkContext.parallelize(data)\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"type(rdd)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"rdd.count()"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"#### Creating RDD referencing a dataset in S3"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"# Retrieve the list of existing buckets\n",
				"\n",
				"import boto3\n",
				"\n",
				"s3 = boto3.client('s3')\n",
				"response = s3.list_buckets()\n",
				"\n",
				"# Output the bucket names\n",
				"print('Existing buckets:')\n",
				"for bucket in response['Buckets']:\n",
				"    print(f'  {bucket[\"Name\"]}')"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"## Replace S3 bucket\n",
				"\n",
				"## rdd = spark.sparkContext.textFile(\"s3://${BUCKET_NAME}/input/lab2/sample.csv\")\n",
				"\n",
				"rdd = spark.sparkContext.textFile(\"s3://===$bucket===/input/lab2/sample.csv\")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"#### RDD Actions "
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"##### count() --> Returns the number of records in an RDD"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"\n",
				"# Action - count\n",
				"\n",
				"print(\"Count : \"+str(rdd.count()))\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"##### first() --> Returns the first record."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"\n",
				"# Action - first\n",
				"\n",
				"firstRec = rdd.first()\n",
				"\n",
				"print(\"First Record : \"+ firstRec)\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"#### Spark RDD Transformations\n",
				"\n",
				"Transfromation performed over Spark are RDD Transformations; which may result to one or more RDDs. RDDs are immutable in nature, transformation always create new RDD without updating the existing one.\n",
				"\n",
				"**Lazy Transformation**\n",
				"\n",
				"RDD Transformations are lazy transformations. It mean none of the operations get executed untill you can call an action on Spark RDD.\n",
				"\n",
				"**RDD Transformation Type**\n",
				"\n",
				"**1. Narrow Transformation**\n",
				"\n",
				"Narrow transformations are the result of map() and filter() functions on the compute data that live on a single partition. That means there is no data movement between partitions to execute narrow transformations.\n",
				"\n",
				"Functions such as map(), mapPartition(), flatMap(), filter(), union() are some examples of narrow transformation\n",
				"\n",
				"**2. Wider/Shuffle transformations**\n",
				"\n",
				"Wider transformations are the result of groupByKey() and reduceByKey() functions on the compute data that live on many partitions. This means there will be data movements between partitions to execute wider transformations. Since these shuffles the data, they also called shuffle transformations.\n",
				"\n",
				"You can learn more about Spark transformation here .\n",
				"\n",
				"filter() Return a new dataset formed by selecting those elements of the source on which function returns true."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"## By using rdd.first(), we have identified the spark RDD rdd which you have just create has first row as header. In this example you are loooking to filter the row as header.\n",
				"\n",
				"rddheader = rdd.first()\n",
				"\n",
				"rddwithoutheader = rdd.filter(lambda c: c != rddheader)\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"#### Here we are showing you - How you can use single record from Spark RDD to perform transformation; and once transformation is complete, you can apply it to the entire dataset."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"#### if you review, you will find each records in rdd are string\n",
				"\n",
				"type(rddwithoutheader.first())"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"#### Step 1 - Convert string into list"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"# reading first record form rdd after removing header\n",
				"s = rddwithoutheader.first()\n",
				"\n",
				"# checkig type\n",
				"type(s)\n",
				"\n",
				"# printing value of s\n",
				"print(s)\n",
				"\n",
				"# converting string into list\n",
				"sl = s.split(',')\n",
				"\n",
				"# checking type\n",
				"type(sl)\n",
				"\n",
				"# printing list\n",
				"for i in sl[0:len(sl)]:\n",
				"    print(i)\n",
				"\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Step 2 - Find distinct Country"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"As rdd.first() shows Country is the second column in list, we can access this by using its index entry (note: index for lists start from 0). The command below will return the country name"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"sl[1]"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"Now let's find all distinct country; for that we are going to use distinct() Spark RDD function.\n",
				"\n",
				"distinct() Return a new dataset that contains the distinct elements of the source dataset."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"rddcountry = rddwithoutheader.map(lambda r : r.split(',')[1]).distinct()\n",
				"\n",
				"rddcountry.count()\n",
				"\n",
				"for i in rddcountry.take(10):\n",
				"    print(i)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"#### Step 3 - filter all the records where country is United Kingdom"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"rddcountryuk = rddwithoutheader.filter(lambda o : o.split(',')[1] in ('United Kingdom'))\n",
				"\n",
				"rddcountryuk.count()\n",
				"\n",
				"for i in rddcountryuk.take(10):\n",
				"    print(i)\n",
				"\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"#### Working with Key-Value Pairs\n",
				"\n",
				"While most Spark operations work on RDDs containing any type of objects, a few special operations are only available on RDDs of key-value pairs. The most common ones are distributed-shuffle-operations, such as grouping or aggregating the elements by a key.\n",
				"\n",
				"In Python, these operations work on RDDs containing built-in Python tuples such as (1, 2). Simply create such tuples and then call your desired operation."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"## convert RDD into KeyValue table; here we have indexed \"Item Type\" and assign 1 to each \"Item Type\"; \n",
				"## this will help us to count each \"Item Type\"\n",
				"\n",
				"rddcountryukkv = rddcountryuk.map(lambda r : (r.split(',')[2],1))\n",
				"\n",
				"# list 10 item_type \n",
				"rddcountryukkv.take(10)\n",
				"\n",
				"## count the total \"Item Type\" by using reduceByKey\n",
				"\n",
				"rddcountryukkvrkey = rddcountryukkv.reduceByKey(lambda a, b: a + b)\n",
				"\n",
				"## print item_type with count\n",
				"\n",
				"for i in rddcountryukkvrkey.take(20):\n",
				"    print(i)\n",
				"\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"#### Sorting the value of Spark RDD"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"**sortByKey([ascending], [numPartitions])** : When called on a dataset of (K, V) pairs where K implements Ordered, returns a dataset of (K, V) pairs sorted by keys in ascending or descending order, as specified in the boolean ascending argument."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"## sorting the Item_Type with count; In Spark RDD you can use sortByKey to perform the sort\n",
				"\n",
				"rddcountryukkvrkeysort = rddcountryukkvrkey.sortByKey()\n",
				"\n",
				"for i in rddcountryukkvrkeysort.take(10):\n",
				"    print(i)\n",
				"\n",
				"# if you see the previous output; it performed the sorting but on Item_Type \n",
				"# not on count; it is because sortByKey perform the sorting on Key instead of\n",
				"# Value i.e. why now we need to perform one transform change the key value pair\n",
				"\n",
				"rddcountryukkvrkey = rddcountryukkvrkey.map(lambda o : (o[1],o[0]))\n",
				"\n",
				"rddcountryukkvrkey.take(10)\n",
				"\n",
				"## Now let's perform the sorting \n",
				"rddcountryukkvrkeysort = rddcountryukkvrkey.sortByKey(False)\n",
				"\n",
				"for i in rddcountryukkvrkeysort.take(10):\n",
				"    print(i)\n",
				"\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Saving the data to S3"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"**saveAsTextFile(path)**: Write the elements of the dataset as a text file (or set of text files) to a given directory in the local filesystem, HDFS or any other Hadoop-supported file system. Spark will call toString on each element to convert it to a line of text in the file"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"## before saving the file to S3 let's do final transform\n",
				"\n",
				"rddcountryukkvrkeysort = rddcountryukkvrkeysort.map(lambda o : (o[1],o[0]))\n",
				"\n",
				"## print the value\n",
				"\n",
				"for i in rddcountryukkvrkeysort.take(10):\n",
				"    print(i)\n",
				"\n",
				"## save the value to s3 as text/CSV file with Gzip compression, replace the bucket name\n",
				"\n",
				"sc._jsc.hadoopConfiguration().set(\"mapred.output.committer.class\", \"org.apache.hadoop.mapred.DirectFileOutputCommitter\")\n",
				"codec = \"org.apache.hadoop.io.compress.GzipCodec\"\n",
				"## rddcountryukkvrkeysort.saveAsTextFile(\"s3a://${BUCKET_NAME}/input/lab2/result\" + \"/part-0000*\" , codec)\n",
				"\n",
				"\n",
				"rddcountryukkvrkeysort.saveAsTextFile(\"s3://===$bucket===/input/lab2/result\" + \"/part-0000*\" , codec)\n",
				"\n",
				"\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"tags": []
			},
			"source": [
				"You have just leared how to malupulate RDD into key,vlaue to perform the Item Type count. Now, we are going to learn how to perform summation in RDD using **reduceByKey()**\n",
				"\n",
				"#### Calcuate sum of total reveue per Item Type"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"# In this part, we are going to calculate sum of total revenue per item type\n",
				"\n",
				"# let's convert the data into key value pair to extract Item Type and Total Revenue  \n",
				"\n",
				"rddcountryukkv = rddcountryuk.map(lambda sl : (sl.split(',')[2],float(sl.split(',')[11])))\n",
				"\n",
				"# perform the sum by using reduceByKey()\n",
				"\n",
				"rddcountryukkvggregateByKey = rddcountryukkv.reduceByKey(lambda total, value : total + value )\n",
				"\n",
				"# print 10 values\n",
				"\n",
				"for i in rddcountryukkvggregateByKey.take(10):\n",
				"    print(i)\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Converting to DataFrame from RDD"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"In next section of this lab; we are working with DataFrames. Let's understand how to can convert RDD into dataframe. The easiest way is to use **rdd.toDF()** function."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"# run one by one\n",
				"type(rddcountryukkvggregateByKey)\n",
				"\n",
				"rddcountryukkvggregateByKeyDF = rddcountryukkvggregateByKey.toDF()\n",
				"\n",
				"type(rddcountryukkvggregateByKeyDF)\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"Since RDD is schema-less without column names and data type, converting from RDD to DataFrame gives you default column names as _1, _2 and so on and data type as String."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"rddcountryukkvggregateByKeyDF.printSchema()"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"Using createDataFrame() from SparkSession is another way to create and it takes rdd object as an argument. and chain with toDF() to specify names to the columns."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"# Create a schema for the dataframe\n",
				"\n",
				"from pyspark.sql.types import StructField, StructType, StringType, IntegerType , FloatType\n",
				"\n",
				"schema = StructType([\n",
				"    StructField('ItemType', StringType(), True),\n",
				"    StructField('SumofTotalRevenue', FloatType(), True)\n",
				"])\n",
				"\n",
				"# Create data frame\n",
				"\n",
				"df = spark.createDataFrame(rddcountryukkvggregateByKey,schema)\n",
				"\n",
				"# show data frame\n",
				"\n",
				"df.show()\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Writing as Parquet to Amazon S3"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"df.write.parquet('s3://===$bucket===/input/lab2/parquetresult/')"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"## Lists all currently running sessions by name and ID.\n",
				"\n",
				"%list_sessions"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"## stop the current session \n",
				"\n",
				"%stop_session"
			]
		}
	],
	"metadata": {
		"kernelspec": {
			"display_name": "Glue PySpark",
			"language": "python",
			"name": "glue_pyspark"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "Python_Glue_Session",
			"pygments_lexer": "python3"
		},
		"toc-autonumbering": true
	},
	"nbformat": 4,
	"nbformat_minor": 4
}
