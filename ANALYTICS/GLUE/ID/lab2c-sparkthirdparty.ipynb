{
	"cells": [
		{
			"cell_type": "markdown",
			"metadata": {
				"deletable": false,
				"editable": false,
				"trusted": true
			},
			"source": [
				"\n",
				"# Glue Studio Notebook\n",
				"You are now running a **Glue Studio** notebook; before you can start using your notebook you *must* start an interactive session.\n",
				"\n",
				"## Available Magics\n",
				"|          Magic              |   Type       |                                                                        Description                                                                        |\n",
				"|-----------------------------|--------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
				"| %%configure                 |  Dictionary  |  A json-formatted dictionary consisting of all configuration parameters for a session. Each parameter can be specified here or through individual magics. |\n",
				"| %profile                    |  String      |  Specify a profile in your aws configuration to use as the credentials provider.                                                                          |\n",
				"| %iam_role                   |  String      |  Specify an IAM role to execute your session with.                                                                                                        |\n",
				"| %region                     |  String      |  Specify the AWS region in which to initialize a session.                                                                                                 |\n",
				"| %session_id                 |  String      |  Returns the session ID for the running session.                                                                                                          |\n",
				"| %connections                |  List        |  Specify a comma separated list of connections to use in the session.                                                                                     |\n",
				"| %additional_python_modules  |  List        |  Comma separated list of pip packages, s3 paths or private pip arguments.                                                                                 |\n",
				"| %extra_py_files             |  List        |  Comma separated list of additional Python files from S3.                                                                                                 |\n",
				"| %extra_jars                 |  List        |  Comma separated list of additional Jars to include in the cluster.                                                                                       |\n",
				"| %number_of_workers          |  Integer     |  The number of workers of a defined worker_type that are allocated when a job runs. worker_type must be set too.                                          |\n",
				"| %glue_version               |  String      |  The version of Glue to be used by this session. Currently, the only valid options are 2.0 and 3.0 (eg: %glue_version 2.0).                               |\n",
				"| %security_config            |  String      |  Define a security configuration to be used with this session.                                                                                            |\n",
				"| %sql                        |  String      |  Run SQL code. All lines after the initial %%sql magic will be passed as part of the SQL code.                                                            |\n",
				"| %streaming                  |  String      |  Changes the session type to Glue Streaming.                                                                                                              |\n",
				"| %etl                        |  String      |  Changes the session type to Glue ETL.                                                                                                                    |\n",
				"| %status                     |              |  Returns the status of the current Glue session including its duration, configuration and executing user / role.                                          |\n",
				"| %stop_session               |              |  Stops the current session.                                                                                                                               |\n",
				"| %list_sessions              |              |  Lists all currently running sessions by name and ID.                                                                                                     |\n",
				"| %worker_type                |  String      |  Standard, G.1X, *or* G.2X. number_of_workers must be set too. Default is G.1X.                                                                           |\n",
				"| %spark_conf                 |  String      |  Specify custom spark configurations for your session. E.g. %spark_conf spark.serializer=org.apache.spark.serializer.KryoSerializer.                      |"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Using 3rd Party Library"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"Replace the `{S3_BUCKET}` below, with your bucket name. Below command is adding required third party libraries to the job."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"%extra_py_files \"s3://{S3_BUCKET}/library/pycountry_convert.zip\""
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"The code below is some boiler-plate imports that will generally be included in the start of every Spark/Glue job and then an import statement for the 3rd party library. Be sure to replace `${S3_BUCKET}` with your bucket name before running the code."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"editable": true,
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"from pyspark.sql.functions import udf, col\n",
				"from pyspark.sql.types import IntegerType, StringType\n",
				"from pyspark import SparkContext\n",
				"from pyspark.sql import SQLContext\n",
				"\n",
				"from datetime import datetime\n",
				"from pycountry_convert import (\n",
				"    convert_country_alpha2_to_country_name,\n",
				"    convert_country_alpha2_to_continent,\n",
				"    convert_country_name_to_country_alpha2,\n",
				"    convert_country_alpha3_to_country_alpha2,\n",
				")\n",
				"\n",
				"s3_path = \"{S3_BUCKET}\"\n",
				"\n",
				"df = spark.read.load(\"s3://\" + s3_path + \"/input/lab2/sample.csv\", \n",
				"                          format=\"csv\", \n",
				"                          sep=\",\", \n",
				"                          inferSchema=\"true\",\n",
				"                          header=\"true\")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"We will define a UDF (user defined function) to use for processing a Spark dataframe. UDFs allow a developer to extend the standard Spark functionality using Python code. To do that your code needs to be in the form of a UDF lambda. The code below creates a Spark UDF `udf_get_country_code2`to convert a country name into a two-letter code."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"def get_country_code2(country_name):\n",
				"    country_code2 = 'US'\n",
				"    try:\n",
				"        country_code2 = convert_country_name_to_country_alpha2(country_name)\n",
				"    except KeyError:\n",
				"        country_code2 = ''\n",
				"    return country_code2\n",
				"\n",
				"udf_get_country_code2 = udf(lambda z: get_country_code2(z), StringType())\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"Next we will create a new dataframe that includes a column created using the UDF we created previously. Notice the new column `country_code_2` in the new dataframe's schema."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"new_df = df.withColumn('country_code_2', udf_get_country_code2(col(\"Country\")))\n",
				"new_df.printSchema()\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"Let's take a look at the data in this new dataframe - notice the new column `country_code_2`. The dataframe now contains two-letter country codes that were determined based on the `Country` column."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"new_df.show(10)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Using Data Catalog"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"So far, we have been running standard Spark code. Now, we will try some Glue-flavored PySpark code. We will now load the tables that we created before in Lab 01 into a Glue dynamic frame. After the data is loaded into a Glue dynamic frame, compare the schema it presented with the schema stored in the Glue Data Catalog table."
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"Notice in the code, we don't specify the S3 location - this is because the Glue Data Catalog knows where the data lives thanks to Glue Data Catalog table definition."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"from awsglue.context import GlueContext\n",
				"\n",
				"glueContext = GlueContext(SparkContext.getOrCreate())\n",
				"\n",
				"dynamic_frame = glueContext.create_dynamic_frame.from_catalog(database=\"console_glueworkshop\", table_name=\"console_csv\")\n",
				"dynamic_frame.printSchema()"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"We can view the data in the **Glue Dynamic Frame** by converting it first to **Data Frame** by calling the `toDF()` function and then using the standard Data Frame `show()` function."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"dynamic_frame.toDF().show(10)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"vscode": {
					"languageId": "plaintext"
				}
			},
			"outputs": [],
			"source": [
				"## stop the current session \n",
				"\n",
				"%stop_session"
			]
		}
	],
	"metadata": {
		"kernelspec": {
			"display_name": "Glue PySpark",
			"language": "python",
			"name": "glue_pyspark"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "Python_Glue_Session",
			"pygments_lexer": "python3"
		}
	},
	"nbformat": 4,
	"nbformat_minor": 4
}
